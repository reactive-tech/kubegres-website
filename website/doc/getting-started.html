<gigo-include-in file="includes/doc-template.html" />

        <div class="contentsBox">
            <div class="contentsBoxHeader">Getting started</div>
            <div class="contentsBoxBody">
                <article>
                    <p>
                        With Kubegres operator, we are going to create a cluster of 3 PostgreSql instances which are replicating
                        their data in real time.
                    </p>

                    <h2>1) Install Kubegres operator</h2>

                    <p>The installation needs to be done once. Run the following command in a Kubernetes cluster:</p>
                    <pre>
kubectl apply -f https://raw.githubusercontent.com/reactive-tech/kubegres/v1.17/kubegres.yaml
                    </pre>

                    <p>During the installation, "Kubegres" creates the namespace "kubegres-system" where it installs its
                        components. Check their status as follows:</p>
                    <pre>
kubectl get all -n kubegres-system
                    </pre>

                    <p>We should see the controller "kubegres-controller-manager" in a running state:</p>
                    <pre>
NAME                                                     STATUS
pod/<b>kubegres-controller-manager</b>-999786dd6-74tmb          <b>Running</b>

NAME                                                    TYPE
service/kubegres-controller-manager-metrics-service     ClusterIP

NAME                                                    READY
deployment.apps/kubegres-controller-manager             1/1

                    </pre>

                    <p>Once it is running, we can check the controller's logs, as follows:</p>
                    <pre>
kubectl logs pod/<b>kubegres-controller-manager</b>-999786dd6-74tmb -c manager -n kubegres-system -f
                    </pre>

                    <h2>2) Check storage class</h2>
                    <p>
                        Kubegres requires a storage class in order to create PV (Persistent Volume) and PVC for each
                        instance of PostgreSql. Please run the following command to check a storage class exist in
                        the Kubernetes cluster:
                    </p>
                    <pre>
kubectl get sc
                    </pre>

                    For example, using <a target="_blank" href="https://kind.sigs.k8s.io/">Kind</a> as a local
                    Kubernetes cluster, the above command outputs:
                    <pre>
NAME                 PROVISIONER             RECLAIMPOLICY
standard (default)   rancher.io/local-path   Delete
                    </pre>

                    <h2>3) Create a Secret resource</h2>

                    <p>Before creating a cluster of PostgreSql, we need to create a Secret resource in order to store
                        the passwords of a PostgreSql's super user and a replication user:</p>
                    <p>Create a file:</p>
                    <pre>
vi my-postgres-secret.yaml
                    </pre>

                    <p>Add the following contents:</p>
                    <pre>
apiVersion: v1
kind: Secret
metadata:
  name: mypostgres-secret
  namespace: default
type: Opaque
stringData:
  <b>superUserPassword: postgresSuperUserPsw
  replicationUserPassword: postgresReplicaPsw</b>
                    </pre>

                    <p>Apply the changes:</p>
                    <pre>
kubectl apply -f my-postgres-secret.yaml
                    </pre>

                    <p>
                        Note that in your Secret YAML, you can set any name that you would like for the keys "superUserPassword"
                        and "replicationUserPassword". The password values in the YAML are also for example purpose.
                    </p>

                    <h2>4) Create a cluster of PostgreSql instances</h2>
                    <p>To create a cluster of PostgreSql, we need to create a Kubegres resource. The Kubegres YAML below
                        contains the minimum required configurations to set-up:
                        <ul>
                            <li>
                                a cluster of PostgreSql pods with an official <a target="_blank" href="https://hub.docker.com/_/postgres">
                                 PostgreSql Docker container</a> (version 12.4 or higher)
                            </li>
                            <li>the property "replica: 3" means, Kubegres will create 1 Primary PostgreSql pod and 2 Replica PostgreSql pods</li>
                            <li>the data will be replicated in real time from the Primary PostgreSql pod to the 2 Replica PostgreSql pods</li>
                        </ul>
                    </p>

                    <p>Create a file:</p>
                    <pre>
vi my-postgres.yaml
                    </pre>

                    <p>Add the following contents:</p>
                    <pre>
apiVersion: kubegres.reactive-tech.io/v1
kind: Kubegres
metadata:
  name: mypostgres
  namespace: default

spec:

   replicas: 3
   image: postgres:16.1

   database:
      size: 200Mi

   env:
      - name: POSTGRES_PASSWORD
        valueFrom:
           secretKeyRef:
              name: mypostgres-secret
              key: superUserPassword

      - name: POSTGRES_REPLICATION_PASSWORD
        valueFrom:
           secretKeyRef:
              name: mypostgres-secret
              key: replicationUserPassword

                    </pre>
                    <p>
                        In the YAML above, under the field "spec.database" we only specified the field "size".
                        But you can also specify the fields "storageClassName" and "volumeMount". There are more details
                        about those fields <a href="/doc/properties-explained.html">in this page.</a>
                        <br>If you do not specify the field "storageClassName", Kubegres finds the default storage class
                        of the Kubernetes cluster and assigns it. But you can also assign a value for that field manually.
                        Once the storageClass is assigned, Kubernetes automatically provisions a PV and a PVC for each Postgres Pod.
                    </p>
                    <p>
                        The YAML above contains the minimum required configurations to deploy a cluster of PostgreSql.
                        There are more configuration options available, such as the possibility to add annotations,
                        set the name of a storageClass, specify a private repo in order to retrieve the container's image from it,
                        enable backup, ...
                        Please see <a href="/doc/properties-explained.html">the full list of options here.</a>
                    </p>

                    <p>Apply the changes:</p>
                    <pre>
kubectl apply -f my-postgres.yaml
                    </pre>

                    <p>
                        A cluster of 3 PostgreSql instances is deploying in Kubernetes. Each PostgreSql instance is a pod.
                        Check their status until they are "Running":
                    </p>
                    <pre>
kubectl get pods -o wide -w
                    </pre>
                    <p>Kubegres logs events about the state of each PostgreSql cluster. Those events provide valuable information
                        and would be very useful to debug any issue. We can access to them, as follows:</p>
                    <pre>
kubectl get events
                    </pre>

                    <h2>5) The created resources</h2>

                <p>Kubegres operator will then create a cluster of PostgreSql pods as follows:</p>
                <p style="text-align: center"><img src="/includes/img/3Replicas.svg" /> </p>

                <p>Let's check the created resources:</p>
                <pre>
kubectl get pod,statefulset,svc,configmap,pv,pvc -o wide
                </pre>

                <p>Example of output:</p>
                <pre>
NAME                 READY   STATUS    NODE
pod/mypostgres-1-0   1/1     Running   worker1
pod/mypostgres-2-0   1/1     Running   worker2
pod/mypostgres-3-0   1/1     Running   worker4

NAME                            READY
statefulset.apps/mypostgres-1   1/1
statefulset.apps/mypostgres-2   1/1
statefulset.apps/mypostgres-3   1/1

NAME                         TYPE
service/mypostgres           ClusterIP
service/mypostgres-replica   ClusterIP

NAME
configmap/base-kubegres-config

NAME                          CAPACITY
persistentvolume/pvc-838...   200Mi
persistentvolume/pvc-da6...   200Mi
persistentvolume/pvc-e25...   200Mi

NAME                                               CAPACITY
persistentvolumeclaim/postgres-db-mypostgres-1-0   200Mi
persistentvolumeclaim/postgres-db-mypostgres-2-0   200Mi
persistentvolumeclaim/postgres-db-mypostgres-3-0   200Mi
                </pre>

                <p>
                     Kubegres has created 3 pods: "mypostgres-1-0", "mypostgres-2-0" and "mypostgres-3-0". Each of those pods
                     runs a PostgreSql DB and are associated to a StatefulSet resource with the same name. And each pod is
                    deployed in a distinct node to ensure strong resiliency in the case of a node failure.
                </p>
                <p>
                    Moreover, it created 2 Kubernetes clusterIP services: "mypostgres" and "mypostgres-replica". Those services allow
                    client apps to access respectively to the Primary and Replica instances. There are more details about
                    them in the next section below.
                </p>
                <p>
                    Kubegres created a base ConfigMap named "
                    <a target="_blank" href="https://github.com/reactive-tech/kubegres/blob/main/internal/controller/spec/template/yaml/BaseConfigMapTemplate.yaml">
                        base-kubegres-config</a>". It does that in each namespace where Kubegres resources are running.
                    For more details about base ConfigMap, please see the page <a href="/doc/override-default-configs.html">Override the default configurations</a>.
                </p>
                    And finally, Kubegres provisioned one PV and one PVC for each Postgres Pod using the StorageClass.
                <p>
                </p>
                <p>
                    Kubegres uses predefined templates to create Kubernetes resources. Those templates are available in
                    <a target="_blank" href="https://github.com/reactive-tech/kubegres/tree/main/internal/controller/spec/template/yaml">
                        GitHub</a>.
                </p>
                <p>
                    From that point, we have a resilient cluster of 3 PostgreSql databases. It is time to connect a
                    client app to that PostgreSql cluster.
                </p>

                <h2>6) Connect client apps to PostgreSql</h2>

                 <p>
                     Based on the Kubegres YAML that we have created, a client app located inside the same Kubernetes
                     cluster would use the following configurations to connect to a PostgreSql database:
                     <ul>
                        <li>host: mypostgres</li>
                        <li>port: 5432</li>
                        <li>username: postgres</li>
                        <li>password: [value of mypostgres-secret/superUserPassword]</li>
                    </ul>
                 </p>

                <h3>Host</h3>
                <p>
                    In this example, Kubegres created 2 Kubernetes Headless services (of default type ClusterIP) using the name defined in YAML
                    (e.g. "mypostgres"):
                    <ul>
                        <li>a Kubernetes service "mypostgres" allowing to access to the Primary PostgreSql instances</li>
                        <li>a Kubernetes service "mypostgres-replica" allowing to access to the Replica PostgreSql instances</li>
                    </ul>
                </p>
                <p>
                    Consequently, a client app running inside a Kubernetes cluster, would use the hostname "mypostgres" to
                    connect to the Primary PostgreSql for read and write requests, and optionally it can also use the
                    hostname "mypostgres-replica" to connect to any of the available Replica PostgreSql for read requests.
                </p>
                <p>
                    This approach allows the client apps to access to the PostgreSql instances without any knowledge of IP addresses.
                </p>
                <p>
                    Note that because Kubegres creates Headless services and configure them with selectors,
                    then no cluster IP addresses are created and the DNS resolves directly to the IPs of the Postgres Pods.
                </p>

                <p>Please see the following diagram showing the 2 services that are created by Kubegres based on the YAML above:</p>

                <p style="text-align: center"><img src="/includes/img/2Services.svg" /> </p>

                <p>
                    If the Primary PostgreSql crashes, the automatic failover
                    process will promote a Replica PostgreSql as a Primary. This process will be transparent for client
                    apps as long as they are using the service name (e.g. "mypostgres") as the hostname. More details
                    about this topic in the doc <a href="/doc/replication-and-failover.html">Replication and failover</a>.
                </p>

                <h3>Port</h3>
                <p>
                    By default, PostgreSql is accessible via the port 5432. It is possible to modify this by adding the
                    property "port" in the YAML above. All possible YAML properties are defined in the doc
                    <a href="/doc/properties-explained.html">all properties explained</a>.
                </p>

                <h3>Username and Password</h3>
                <p>
                    By default, the only available user is "postgres" which is a super user.
                    Consequently, for your custom databases it is recommended to create an additional user with limited
                    permissions, for example with specific access permissions to a set of database(s). It is possible to do so by creating:
                    <ul>
                        <li>
                            an environment variable which contains the password of your custom PostgreSql user,
                            as explained <a href="/doc/set-env-vars.html">here</a>.
                        </li>
                        <li>
                            a ConfigMap where we can override the bash script "primary_init_script.sh"
                            as shown <a href="/doc/override-default-configs.html#primary_init_script">here</a>. In that bash script,
                            it is possible to execute any SQL queries to create custom database(s), user(s) and anything
                            else required to initialise PostgreSql.
                        </li>
                    </ul>
                </p>

                <h2>7) Delete a cluster of Postgres</h2>
                <p>
                    A cluster of Postgres can be deleted with the command:
                    <pre>
kubectl delete kubegres [unique name]
                    </pre>
                    For example:
                    <pre>
kubectl delete kubegres mypostgres
                    </pre>

                    The above will delete all resources created for a cluster of Postgres identified by the name 'mypostgres'.
                    It will delete resources such as Pods, Statefulsets, Services, ...
                    The only resources that you have to remove manually are PV and PVC. There is one PV and one PVC created
                    for each Pod. The reason why we don't to remove those resources is because they contain the database.
                    In a future version of Kubegres it will be possible to set an option in the YAML so that they are
                    automatically removed too.
                </p>
                <p>Note that the command above will not delete other clusters of Postgres created with Kubegres.
                    You have to run the command above with the name of the Kubegres resource for each cluster of Postgres to delete.</p>

                <p>The above command does not delete the Kubegres controller/operator from your cluster.
                    If you would like to delete it, you can run:</p>
                <pre>
kubectl delete -f https://raw.githubusercontent.com/reactive-tech/kubegres/v1.17/kubegres.yaml
                </pre>

                <h2>Next</h2>
                <p>
                    You can read about all properties that you can use in a YAML of "kind: Kubegres" in the page: <a href="properties-explained.html">All properties explained</a>.
                </p>
                <p>
                    You can read about how Kubegres manages <a href="replication-and-failover.html">replication and failover</a>.
                </p>

                    <div class="publicationDetails"><time datetime="2021-12-17" pubdate="pubdate">17 December 2021</time></div>

                </article>
            </div>
        </div>
